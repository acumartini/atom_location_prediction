\section{Lessons Learned}
\label{lessons}

\subsection{Simulation}

The initial plan of computing ray interference was unfortunately not completed due to time constraints, but the fully classical raytracer works as well as can be expected and is quite fast.

One interesting aspect is that the system is a serial implementation, but high concurrency can be attained by running multiple threads using \verb|qsub| while scriping different arguments. While this is perhaps the most na\"ive way to implement parallelization, it is also highly effective for a massively parallel task, and was far more efficient than similar setups implemented with MPI or Cilk. It seems like this would be an effective way to introduce parallelization, since the perfomance gains are immediately clear.


\subsection{Data Preprocessing}

%Ran talks about filtering here...
Sharing the idea and details of the implementation of Poisson distribution based noise maker is quite a job. Due to lack of base support of Poisson distribution random number generator, a float number and the range of the floating number is quite interesting topic. The final result turns out to be a integer generator with a scaling shifting. Hence, we can satisfy our purpose and simplify the implementation at same time.

%Adam talks about feature scaling here...
To accomplish distributed feature scaling and label formating we need to need to create a custom MPI reduce operation that is capable of merging two sets of labels into a single unique set.  We learned that creating custom reducers can be the source of many bugs related to incorrect data pointers.  This was also the case when created custom reducers for gradient updates, which we discuss below.

\subsection{Machine Learning}

%Ran talks about filtering here...
The origin plan includes another distributed memory system based classifier, which is distributed Support Vector Machine (SVM). The distributed SVM is a backup as a classifier for training and testing the ray images. However, after three weeks of work, the result turns out to be unacceptable. The implementation of distributed SVM contains a large number of interfaces that allows user to train and test the result. The only function our project needs is a regression mode for the 100 times 100 pixels image classifier. However, the regression mode does not support such simple functions after parsing the SVM Light files. The implementation complexes this part into more detailed partial differential equation classifier. Therefore, the decision between continuing the work to implement a corresponding function and stop to switch focusing on other object becomes quite controversial. Due to time limit and tight schedule of my last three weeks. Lacking of machine learning knowledge, we spend too much time studying on basic concept of SVM. The lesson we learned is to better schedule the timeline and estimate the workload for a totally new area.

There were several opportunities for learning during development of the distributed logistic regression classifier.  As mentioned above, there were many bugs surrounding custom reduce functions and different implementations of Allreduce on different systems.  The was one case where reduce was working correctly on ix-trusty and not on ACISS.  However, these issued where solved by replacing custom functions, which used linear algebra library operations, with standard MPI reducers.  Therefor, we learned that it is best to use standard MPI functions for operations whenever possible to enable cross-platform compatibility.

Another important lesson came from designing the classifier to be flexible and scalable so that it can easily be applied to various size systems and datasets.  The first challenge was deciding how to partition the data based on task ID and the number of computation nodes.  Our initial design read from separate data folders such that the user was required to preprocess the data based on how they want the computation to be divided.  However, we quickly realized that the division of data should be handled by the classifier at runtime.  We accomplished this by allowing the user to place all data into a single folder and the loading the data on each node according to task ID.  This method assumes that all data instance are roughly equal in size, which is a safe assumption given the homogeneous nature of a classification problem.

One interesting lesson we learned about distributed classification is that data must be extremely large for mini-batching to apply.  Distributed classification is already \emph{batching} the data across nodes, and mini-batching further divides these batches.  Therefore, the dataset must significantly exceed each nodes' computational capacity before mini-batching will be necessary.
